import traceback
from typing import Any, Dict, NamedTuple, Optional, Tuple

import pandas as pd
import torch
import torch.fx
from torch._dispatch.python import enable_python_dispatcher
from torch._guards import detect_fake_mode
from torch._subclasses.meta_utils import is_sparse_any
from torch.fx._compatibility import compatibility
from torch.fx.node import map_aggregate, Node
from compute_graph_analysis import utils
__all__ = ["TensorMetadata", "ShapeProp"]








@compatibility(is_backward_compatible=True)
class TensorMetadata(NamedTuple):
    # TensorMetadata is a structure containing pertinent information
    # about a tensor within a PyTorch program.

    # General Tensor metadata
    shape: torch.Size = None
    dtype: torch.dtype = None
    requires_grad: bool = False
    stride: Tuple[int, ...] = None
    memory_format: Optional[torch.memory_format] = None

    # Quantization metadata
    is_quantized: bool = False
    qparams: Dict[str, Any] = {}


def _extract_tensor_metadata(
        result: torch.Tensor,
        include_contiguity=True) -> TensorMetadata:
    """
        Extract a TensorMetadata NamedTuple describing `result`.
        """
    shape = result.shape
    dtype = result.dtype
    requires_grad = result.requires_grad
    stride = result.stride() if not is_sparse_any(result) else ()

    memory_format = None

    if include_contiguity and not is_sparse_any(result):
        '''
        åœ¨ PyTorch ä¸­ï¼Œtorch.contiguous_format, torch.channels_last, å’Œ torch.channels_last_3d æ˜¯ä¸Žå¼ é‡å†…å­˜å¸ƒå±€ç›¸å…³çš„æ¦‚å¿µã€‚è¿™äº›æ ¼å¼å®šä¹‰äº†å¤šç»´å¼ é‡ï¼ˆå°¤å…¶æ˜¯å›¾åƒæ•°æ®ï¼‰åœ¨å†…å­˜ä¸­çš„å­˜å‚¨æ–¹å¼ï¼Œè¿™å¯¹äºŽä¼˜åŒ–æŸäº›ç‰¹å®šæ“ä½œï¼ˆå¦‚å·ç§¯è¿ç®—ï¼‰çš„æ€§èƒ½éžå¸¸é‡è¦ã€‚ä¸‹é¢æ˜¯å¯¹è¿™ä¸‰ç§å†…å­˜æ ¼å¼çš„è§£é‡Šï¼š

        torch.contiguous_format:
        è¿™æ˜¯é»˜è®¤çš„å†…å­˜å¸ƒå±€æ ¼å¼ã€‚åœ¨è¿™ç§æ ¼å¼ä¸‹ï¼Œå¼ é‡çš„å…ƒç´ åœ¨å†…å­˜ä¸­æ˜¯è¿žç»­å­˜å‚¨çš„ï¼Œå¹¶ä¸”éµå¾ªCè¯­è¨€é£Žæ ¼çš„é¡ºåºï¼ˆè¡Œä¼˜å…ˆï¼‰ã€‚è¿™æ„å‘³ç€å¯¹äºŽä¸€ä¸ªå½¢çŠ¶ä¸º (N, C, H, W) çš„4Då¼ é‡ï¼Œå…¶å†…å­˜å¸ƒå±€æ˜¯æŒ‰ç…§ Nã€Cã€Hã€W çš„é¡ºåºä¾æ¬¡æŽ’åˆ—ã€‚
        å½“ä½¿ç”¨è¿™ç§æ ¼å¼æ—¶ï¼Œå¼ é‡é€šå¸¸æ˜¯â€œè¿žç»­â€çš„ï¼Œå³æ¯ä¸ªç»´åº¦ä¸Šçš„æ­¥å¹…ï¼ˆstrideï¼‰åæ˜ äº†å¼ é‡çš„å®žé™…å¤§å°ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽä¸€ä¸ªå½¢çŠ¶ä¸º (2, 3, 4, 5) çš„å¼ é‡ï¼Œå…¶æ­¥å¹…å¯èƒ½æ˜¯ (60, 20, 5, 1)ã€‚
        torch.channels_last:
        è¿™ç§æ ¼å¼ä¸»è¦ç”¨äºŽäºŒç»´å›¾åƒæ•°æ®ï¼ˆé€šå¸¸æ˜¯å½¢çŠ¶ä¸º (N, C, H, W) çš„å¼ é‡ï¼‰ï¼Œå…¶ä¸­é€šé“ï¼ˆCï¼‰ç»´åº¦è¢«ç§»åŠ¨åˆ°æœ€åŽã€‚å› æ­¤ï¼Œå†…å­˜å¸ƒå±€å˜æˆäº† (N, H, W, C)ã€‚
        ä½¿ç”¨è¿™ç§æ ¼å¼å¯ä»¥åŠ é€ŸæŸäº›ç±»åž‹çš„å·ç§¯æ“ä½œï¼Œå› ä¸ºçŽ°ä»£GPUæž¶æž„é€šå¸¸å¯¹è¿™ç§å¸ƒå±€è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰§è¡Œå·ç§¯æ—¶ï¼Œå®ƒå¯ä»¥å‡å°‘å†…å­˜è®¿é—®æ¨¡å¼çš„è·³è·ƒæ€§ï¼Œä»Žè€Œæé«˜ç¼“å­˜æ•ˆçŽ‡å’Œè®¡ç®—é€Ÿåº¦ã€‚
        è¦å°†å¼ é‡è½¬æ¢ä¸ºæ­¤æ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨ .to(memory_format=torch.channels_last) æ–¹æ³•ã€‚
        torch.channels_last_3d:
        ç±»ä¼¼äºŽ torch.channels_lastï¼Œä½†é€‚ç”¨äºŽä¸‰ç»´å›¾åƒæ•°æ®ï¼ˆé€šå¸¸æ˜¯å½¢çŠ¶ä¸º (N, C, D, H, W) çš„å¼ é‡ï¼‰ï¼Œå…¶ä¸­é€šé“ï¼ˆCï¼‰ç»´åº¦ä¹Ÿè¢«ç§»åŠ¨åˆ°æœ€åŽã€‚å› æ­¤ï¼Œå†…å­˜å¸ƒå±€å˜ä¸º (N, D, H, W, C)ã€‚
        è¿™ç§å¸ƒå±€åŒæ ·æ˜¯ä¸ºäº†ä¼˜åŒ–ä¸‰ç»´å·ç§¯ç­‰æ“ä½œï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†é¢‘æˆ–ä½“ç§¯æ•°æ®æ—¶ã€‚é€šè¿‡è°ƒæ•´å†…å­˜å¸ƒå±€ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨GPUçš„ç¡¬ä»¶ç‰¹æ€§æ¥åŠ é€Ÿè®¡ç®—ã€‚
        åŒæ ·åœ°ï¼Œè¦å°†å¼ é‡è½¬æ¢ä¸ºæ­¤æ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨ .to(memory_format=torch.channels_last_3d) æ–¹æ³•ã€‚
        '''
        memory_formats = {
            torch.contiguous_format,
            torch.channels_last,
            torch.channels_last_3d,
        }
        for query_format in memory_formats:
            if result.is_contiguous(memory_format=query_format):
                memory_format = query_format
                break

    is_quantized = result.is_quantized

    qparams: Dict[str, Any] = {}

    if is_quantized:
        qscheme = result.qscheme()
        '''
        torch.per_tensor_affine å’Œ torch.per_tensor_symmetric æ˜¯ PyTorch ä¸­ä¸Žé‡åŒ–ç›¸å…³çš„æ¦‚å¿µï¼Œå®ƒä»¬æ˜¯ä¸¤ç§ä¸åŒçš„é‡åŒ–æ–¹æ¡ˆï¼Œç”¨äºŽåœ¨æ¨¡åž‹é‡åŒ–è¿‡ç¨‹ä¸­å®šä¹‰å¦‚ä½•å¯¹å¼ é‡è¿›è¡Œé‡åŒ–ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½å±žäºŽé€å¼ é‡ï¼ˆper-tensorï¼‰é‡åŒ–ç­–ç•¥ï¼Œæ„å‘³ç€æ•´ä¸ªå¼ é‡ä½¿ç”¨ç›¸åŒçš„é‡åŒ–å‚æ•°ï¼ˆå¦‚ç¼©æ”¾å› å­å’Œé›¶ç‚¹ï¼‰ã€‚ä¸‹é¢æ˜¯è¿™ä¸¤ä¸ªæœ¯è¯­çš„ç®€è¦è§£é‡Šï¼š

        torch.per_tensor_affine:
        è¿™ç§é‡åŒ–æ–¹å¼ä½¿ç”¨ä»¿å°„å˜æ¢æ¥å°†æµ®ç‚¹å€¼æ˜ å°„åˆ°æ•´æ•°è¡¨ç¤ºã€‚å®ƒé€‚ç”¨äºŽé‚£äº›åˆ†å¸ƒä¸å¯¹ç§°çš„æ•°æ®ã€‚
        é‡åŒ–å…¬å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š
        ð‘„=round(ð‘‹/ð‘ ð‘ð‘Žð‘™ð‘’+ð‘§ð‘’ð‘Ÿð‘œ_ð‘ð‘œð‘–ð‘›ð‘¡)
        Q=round(X/scale+zero_point)ï¼Œå…¶ä¸­ ð‘‹ æ˜¯åŽŸå§‹æµ®ç‚¹æ•°å€¼ï¼Œ
        scale æ˜¯ç¼©æ”¾å› å­ï¼Œ
        zero_point æ˜¯ä¸€ä¸ªæ•´æ•°åç§»é‡ï¼Œç”¨æ¥ä¿è¯é‡åŒ–åŽçš„å€¼èƒ½å¤Ÿå‡†ç¡®åœ°è¡¨ç¤º0é™„è¿‘çš„å€¼ã€‚
        è¯¥æ–¹æ³•é€šå¸¸ç”¨äºŽæ¿€æ´»å‡½æ•°çš„è¾“å‡ºæˆ–æƒé‡ç­‰éœ€è¦ä¿æŒæ­£è´Ÿå€¼çš„æƒ…å†µã€‚

        torch.per_tensor_symmetric:
        å¯¹äºŽæ•°æ®åˆ†å¸ƒç›¸å¯¹å¯¹ç§°çš„æƒ…å†µï¼Œå¯ä»¥é‡‡ç”¨å¯¹ç§°é‡åŒ–æ–¹æ³•ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œé›¶ç‚¹é€šå¸¸æ˜¯0ï¼Œè¿™æ„å‘³ç€é‡åŒ–è¿‡ç¨‹æ˜¯å¯¹åŽŸç‚¹å¯¹ç§°çš„ã€‚
        é‡åŒ–å…¬å¼ç®€åŒ–ä¸ºï¼šð‘„=round(ð‘‹/ð‘ ð‘ð‘Žð‘™ð‘’)
        Q=round(X/scale)ï¼Œè¿™é‡Œæ²¡æœ‰æ˜¾å¼çš„é›¶ç‚¹åç§»ã€‚
        ç”±äºŽå¯¹ç§°æ€§çš„ç‰¹ç‚¹ï¼Œè¿™ç§æ–¹æ³•é€šå¸¸ç”¨äºŽæƒé‡çš„é‡åŒ–ï¼Œå°¤å…¶æ˜¯å½“æƒé‡çš„åˆ†å¸ƒæŽ¥è¿‘å¯¹ç§°æ—¶ï¼Œå¯ä»¥å‡å°‘é‡åŒ–å¸¦æ¥çš„è¯¯å·®ã€‚
        åœ¨PyTorchä¸­ï¼Œè¿™äº›é‡åŒ–æ¨¡å¼é€šå¸¸ä½œä¸ºé‡åŒ–é…ç½®çš„ä¸€éƒ¨åˆ†è¢«æŒ‡å®šï¼Œä¾‹å¦‚åœ¨åˆ›å»ºé‡åŒ–è§‚æµ‹å™¨ï¼ˆQuantization Observerï¼‰æ—¶ã€‚è§‚æµ‹å™¨ä¼šæ ¹æ®ç»™å®šçš„æ¨¡å¼è®¡ç®—å‡ºåˆé€‚çš„é‡åŒ–å‚æ•°ï¼Œå¹¶åº”ç”¨äºŽåŽç»­çš„é‡åŒ–æ“ä½œã€‚
        '''
        qparams["qscheme"] = qscheme


        if qscheme in {torch.per_tensor_affine, torch.per_tensor_symmetric}:
            qparams["scale"] = result.q_scale()  # type: ignore[assignment]
            qparams["zero_point"] = result.q_zero_point()  # type: ignore[assignment]
        elif qscheme in {
            torch.per_channel_affine,
            torch.per_channel_affine_float_qparams,
            torch.per_channel_symmetric,
        }:
            '''
            åœ¨ PyTorch ä¸­ï¼Œtorch.per_channel_affine, torch.per_channel_affine_float_qparams, \
            å’Œ torch.per_channel_symmetric æ˜¯ä¸Žé‡åŒ–ç›¸å…³çš„å‚æ•°é…ç½®æ–¹æ¡ˆã€‚
            è¿™äº›é…ç½®å®šä¹‰äº†å¦‚ä½•å¯¹å¼ é‡è¿›è¡Œé€é€šé“ï¼ˆper-channelï¼‰é‡åŒ–ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å·ç§¯å±‚æƒé‡ç­‰åœºæ™¯ã€‚ä¸‹é¢æ˜¯å¯¹è¿™ä¸‰ç§é‡åŒ–æ–¹æ¡ˆçš„è§£é‡Šï¼š

            1.torch.per_channel_affine:
            è¿™ç§é‡åŒ–æ–¹å¼ä½¿ç”¨é€é€šé“ä»¿å°„å˜æ¢æ¥å°†æµ®ç‚¹å€¼æ˜ å°„åˆ°æ•´æ•°è¡¨ç¤ºã€‚æ¯ä¸ªé€šé“éƒ½æœ‰è‡ªå·±çš„ç¼©æ”¾å› å­ï¼ˆscaleï¼‰å’Œé›¶ç‚¹ï¼ˆzero_pointï¼‰ï¼Œ
            è¿™æ„å‘³ç€ä¸åŒé€šé“å¯ä»¥æ ¹æ®å…¶æ•°æ®åˆ†å¸ƒç‹¬ç«‹åœ°è¿›è¡Œé‡åŒ–ã€‚
            é‡åŒ–å…¬å¼ä¸ºï¼š
            ð‘„=round(ð‘‹/ð‘ ð‘ð‘Žð‘™ð‘’+ð‘§ð‘’ð‘Ÿð‘œ_ð‘ð‘œð‘–ð‘›ð‘¡)
            Q=round(X/scale+zero_point)ï¼Œå…¶ä¸­ ð‘‹æ˜¯åŽŸå§‹æµ®ç‚¹æ•°å€¼ï¼Œ
            scale å’Œ zero_point éƒ½æ˜¯é’ˆå¯¹æ¯ä¸ªé€šé“å•ç‹¬ç¡®å®šçš„ã€‚
            è¿™ç§æ–¹æ³•é€šå¸¸ç”¨äºŽå·ç§¯å±‚çš„æƒé‡ï¼Œå› ä¸ºä¸åŒè¾“å‡ºé€šé“çš„æ•°æ®åˆ†å¸ƒå¯èƒ½éžå¸¸ä¸åŒï¼Œé‡‡ç”¨é€é€šé“é‡åŒ–å¯ä»¥æ›´å¥½åœ°ä¿ç•™ä¿¡æ¯ã€‚

            2.torch.per_channel_affine_float_qparams:
            è¿™ç§é‡åŒ–æ–¹å¼ç±»ä¼¼äºŽ torch.per_channel_affineï¼Œ
            ä½†å…è®¸ç¼©æ”¾å› å­ï¼ˆscaleï¼‰å’Œé›¶ç‚¹ï¼ˆzero_pointï¼‰ä»¥æµ®ç‚¹æ•°çš„å½¢å¼å­˜å‚¨ã€‚è¿™å¯¹äºŽéœ€è¦é«˜ç²¾åº¦é‡åŒ–å‚æ•°çš„æƒ…å†µç‰¹åˆ«æœ‰ç”¨ã€‚
            ç”±äºŽé‡åŒ–å‚æ•°æ˜¯æµ®ç‚¹æ•°ï¼Œè¿™ç§æ–¹æ³•ä¸»è¦ç”¨äºŽé‚£äº›éœ€è¦è¾ƒé«˜ç²¾åº¦çš„æƒ…å†µï¼Œå°¤å…¶æ˜¯åœ¨æŸäº›ç‰¹å®šç¡¬ä»¶å¹³å°ä¸Šã€‚
            å®ƒåŒæ ·é€‚ç”¨äºŽå·ç§¯å±‚çš„æƒé‡ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ›´ç²¾ç»†æŽ§åˆ¶é‡åŒ–å‚æ•°çš„æƒ…å†µä¸‹ã€‚

            3. torch.per_channel_symmetric:
            è¿™æ˜¯ä¸€ç§é€é€šé“å¯¹ç§°é‡åŒ–çš„æ–¹æ³•ï¼Œå‡è®¾æ¯ä¸ªé€šé“çš„æ•°æ®åˆ†å¸ƒæ˜¯å¯¹ç§°çš„ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œé›¶ç‚¹é€šå¸¸æ˜¯0ï¼Œå› æ­¤é‡åŒ–è¿‡ç¨‹æ˜¯å¯¹åŽŸç‚¹å¯¹ç§°çš„ã€‚
            é‡åŒ–å…¬å¼ç®€åŒ–ä¸ºï¼š
            ð‘„=round(ð‘‹/ð‘ ð‘ð‘Žð‘™ð‘’)
            Q=round(X/scale)ï¼Œè¿™é‡Œæ²¡æœ‰æ˜¾å¼çš„é›¶ç‚¹åç§»ã€‚
            å¯¹ç§°é‡åŒ–ç‰¹åˆ«é€‚åˆäºŽæƒé‡çš„é‡åŒ–ï¼Œå°¤å…¶æ˜¯å½“æƒé‡çš„åˆ†å¸ƒæŽ¥è¿‘å¯¹ç§°æ—¶ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å‡å°‘é‡åŒ–å¸¦æ¥çš„è¯¯å·®ã€‚
            '''

            # In this branch, scale and zero_point are expected to be tensors,
            # we store the values as immutable_list in TensorMetadata for
            # easier serialization downstream



            qparams["scale"] = result.q_per_channel_scales().tolist()  # type: ignore[assignment]
            qparams["zero_point"] = result.q_per_channel_zero_points().tolist()  # type: ignore[assignment]
            qparams["axis"] = result.q_per_channel_axis()  # type: ignore[assignment]

    return TensorMetadata(
        shape, dtype, requires_grad, stride, memory_format, is_quantized, qparams
    )


@compatibility(is_backward_compatible=True)
class ShapeProp(torch.fx.Interpreter):
    """
    Execute an FX graph Node-by-Node and
    record the shape and type of the result
    into the corresponding node.

    Example:
         In this example, we record the shape
         and data type of a module given
         an example input ``torch.randn(50, D_in)``.
         We print the name, shape and dtype of each node.

        class TwoLayerNet(torch.nn.Module):
            def __init__(self, D_in, H, D_out):
                super().__init__()
                self.linear1 = torch.nn.Linear(D_in, H)
                self.linear2 = torch.nn.Linear(H, D_out)
            def forward(self, x):
                h_relu = self.linear1(x).clamp(min=0)
                y_pred = self.linear2(h_relu)
                return y_pred
        N, D_in, H, D_out = 64, 1000, 100, 10
        x = torch.randn(N, D_in)
        y = torch.randn(N, D_out)
        model = TwoLayerNet(D_in, H, D_out)
        gm = torch.fx.symbolic_trace(model)
        sample_input = torch.randn(50, D_in)
        ShapeProp(gm).propagate(sample_input)

        for node in gm.graph.nodes:
            print(node.name, node.meta['tensor_meta'].dtype,
                node.meta['tensor_meta'].shape)

        The output of this code is:

        x torch.float32 torch.Size([50, 1000])
        linear1 torch.float32 torch.Size([50, 100])
        clamp_1 torch.float32 torch.Size([50, 100])
        linear2 torch.float32 torch.Size([50, 10])
        output torch.float32 torch.Size([50, 10])

    Args:
         module (GraphModule): The module to be executed
         fake_mode (FakeTensorMode): A fake mode for copying the gm

    """

    def __init__(self, gm, fake_mode=None):
        super().__init__(gm)
        self.valid_metas = ["shape","dtype",
                            "requires_grad","stride",
                            "memory_format","is_quantized","qparams"]
        if fake_mode is None:
            fake_mode = detect_fake_mode()
        if fake_mode is not None:
            from torch._dynamo.utils import deepcopy_to_fake_tensor

            # Note:
            # We need fake execution cause the inputs are fake, however, we cannot fakify the module
            # - because we need to write to the tensor_meta of the real module. So we fakify to
            # produce a result (L131 below), to extract tensor meta, and then keep going.
            #
            # If we were to fakify, we would write to the wrong node, and then downstream fusion
            # would be missing the tensor_meta.
            #
            # See torch/_inductor/overrides.py for where this is called upstream of fusion.
            self.fake_module = deepcopy_to_fake_tensor(self.module, fake_mode)
            self.fake_mode = fake_mode
        else:
            self.fake_module = None
            self.fake_mode = None

        self.real_module = self.module

    def find_aim_meta_list(self,pio_meta_list):
        aim_meta_list = ["shape","dtype"]
        for pm in pio_meta_list:
            if pm:
                if pm.strip().lower() in self.valid_metas:
                    aim_meta_list.append(pm)

        aim_meta_list = list(set(aim_meta_list))

        return aim_meta_list[:]

    def run_node(self, n: Node) -> Any:
        try:
            if self.fake_module is not None:
                # Hacky swap. Alternatively, we could do this with overriding
                # call_module and get_attr.
                self.module = self.fake_module
            try:
                if self.fake_mode is not None:
                    with self.fake_mode, enable_python_dispatcher():
                        result = super().run_node(n)
                else:
                    result = super().run_node(n)
            finally:
                self.module = self.real_module
        except Exception as e:
            traceback.print_exc()
            raise RuntimeError(
                f"ShapeProp error for: node={n.format_node()} with " f"meta={n.meta}"
            ) from e

        found_tensor = False

        def extract_tensor_meta(obj):
            if isinstance(obj, torch.Tensor):
                nonlocal found_tensor
                found_tensor = True
                return _extract_tensor_metadata(obj)
            else:
                return obj

        meta = map_aggregate(result, extract_tensor_meta)
        if found_tensor:
            n.meta["tensor_meta"] = meta

        n.meta["type"] = type(result)
        return result

    def propagate(self, *args):
        """
        Run `module` via interpretation and return the result and
        record the shape and type of each node.

        Args:
            *args (Tensor): the sample input.

        Returns:
            Any: The value returned from executing the Module
        """
        if self.fake_mode is not None:
            fake_args = [
                self.fake_mode.from_tensor(t) if isinstance(t, torch.Tensor) else t
                for t in args
            ]
        else:
            fake_args = args
        return super().run(*fake_args)

    def catch_graph_tensor_prop(self, aim_meta_list=["shape", "dtype"]):
        total_out_res_pdf = pd.DataFrame()
        graph = self.graph

        for node in graph.nodes:
            node_res_pdf = utils.catch_node_tensor_prop(node, aim_meta_list=aim_meta_list)
            if total_out_res_pdf.shape[0] < 1:
                total_out_res_pdf = node_res_pdf.copy().reset_index(drop=True)
            else:
                total_out_res_pdf = pd.concat([total_out_res_pdf,
                                               node_res_pdf.reset_index(drop=True)], axis=0).reset_index(drop=True)
        return total_out_res_pdf


class ShapeProp2(torch.fx.Interpreter):
    """
    Shape propagation. This class takes a `GraphModule`.
    Then, its `propagate` method executes the `GraphModule`
    node-by-node with the given arguments. As each operation
    executes, the ShapeProp class stores away the shape and
    element type for the output values of each operation on
    the `shape` and `dtype` attributes of the operation's
    `Node`.
    """

    def __init__(self, mod):
        self.mod = mod
        self.graph = mod.graph
        self.modules = dict(self.mod.named_modules())

    def propagate(self, *args):
        args_iter = iter(args)
        env: Dict[str, Node] = {}

        def load_arg(a):
            return torch.fx.graph.map_arg(a, lambda n: env[n.name])

        def fetch_attr(target: str):
            target_atoms = target.split('.')
            attr_itr = self.mod
            for i, atom in enumerate(target_atoms):
                if not hasattr(attr_itr, atom):
                    raise RuntimeError(f"Node referenced nonexistent target {'.'.join(target_atoms[:i])}")
                attr_itr = getattr(attr_itr, atom)
            return attr_itr

        for node in self.graph.nodes:
            if node.op == 'placeholder':
                result = next(args_iter)
            elif node.op == 'get_attr':
                result = fetch_attr(node.target)
            elif node.op == 'call_function':
                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))
            elif node.op == 'call_method':
                self_obj, *args = load_arg(node.args)
                kwargs = load_arg(node.kwargs)
                result = getattr(self_obj, node.target)(*args, **kwargs)
            elif node.op == 'call_module':
                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))

            # This is the only code specific to shape propagation.
            # you can delete this `if` branch and this becomes
            # a generic GraphModule interpreter.
            if isinstance(result, torch.Tensor):
                node.shape = result.shape
                node.dtype = result.dtype

            env[node.name] = result

        return load_arg(self.graph.result)









